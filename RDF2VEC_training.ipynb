{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip, os, csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import networkx as nx\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d693a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0dc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    sc.stop()\n",
    "\n",
    "config = SparkConf()\n",
    "config.setMaster(\"local\")\n",
    "config.set(\"spark.executor.memory\", \"70g\")\n",
    "config.set('spark.driver.memory', '90g')\n",
    "config.set(\"spark.memory.offHeap.enabled\",True)\n",
    "config.set(\"spark.memory.offHeap.size\",\"50g\") \n",
    "config.set(\"spark.shuffle.blockTransferService\", \"nio\")\n",
    "sc = SparkContext(conf=config)\n",
    "print (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc349466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"spark version=\" ,sc.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196bd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the links (neighbors) of a given source node in the network\n",
    "def getLinks(net, source):\n",
    "    # If the source node is not found in the dictionary, return an empty dictionary,\n",
    "    # indicating that the node has no links.\n",
    "    if source not in net:\n",
    "        return {}\n",
    "    return net[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5905a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a random walk with uniform probability on a network\n",
    "def randomWalkUniform(triples, startNode, max_depth=5):\n",
    "    next_node = startNode\n",
    "    path = 'n' + str(startNode) + '->'  # Initialize the path with the starting node\n",
    "    for i in range(max_depth):\n",
    "        neighs = getLinks(triples, next_node)  # Get neighbors of the current node\n",
    "        if len(neighs) == 0:\n",
    "            break  # If no neighbors, break the loop\n",
    "\n",
    "        weights = []\n",
    "        queue = []\n",
    "\n",
    "        # Create a queue of edges and their corresponding neighbor nodes\n",
    "        for neigh in neighs:\n",
    "            for edge in neighs[neigh]:\n",
    "                queue.append((edge, neigh))\n",
    "                weights.append(1.0)  # Assign equal weights to all edges\n",
    "\n",
    "        # Randomly choose an edge and its corresponding neighbor node\n",
    "        edge, next_node = random.choice(queue)\n",
    "\n",
    "        # Append the chosen edge and neighbor node to the path\n",
    "        path = path + 'e' + str(edge) + '->'\n",
    "        path = path + 'n' + str(next_node) + '->'\n",
    "\n",
    "    return path  # Return the path generated during the random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(folders, filename):\n",
    "    # Initialize dictionaries to store mappings and triples\n",
    "    entity2id = {}  # Entity to ID mapping\n",
    "    relation2id = {}  # Relation to ID mapping\n",
    "    triples = {}  # Triples in the form of (head, tail, relation)\n",
    "\n",
    "    ent_counter = 0  # Counter for entities\n",
    "    rel_counter = 0  # Counter for relations\n",
    "\n",
    "    # Loop through the provided directories\n",
    "    for dirname in folders:\n",
    "        for fname in os.listdir(dirname):\n",
    "            # Check if the filename is contained in the current file\n",
    "            if not filename in fname:\n",
    "                continue\n",
    "            print(fname)\n",
    "            \n",
    "            # Open and read the GZipped file\n",
    "            gzfile = gzip.open(os.path.join(dirname, fname), mode='rt', errors='ignore')\n",
    "\n",
    "            # Iterate over lines in the file\n",
    "            for line in csv.reader(gzfile, delimiter=' ', quotechar='\"'):\n",
    "                h = line[0]  # Head entity\n",
    "                r = line[1]  # Relation\n",
    "                t = line[2]  # Tail entity\n",
    "\n",
    "                # Check if the tail entity starts with '<' (usually denoting an entity)\n",
    "                if not t.startswith('<'):\n",
    "                    continue\n",
    "\n",
    "                # Skip triples with a specific relation\n",
    "                if 'ddi-interactor-in' in r:\n",
    "                    continue\n",
    "\n",
    "                # Handle the head entity\n",
    "                if h in entity2id:\n",
    "                    hid = entity2id[h]  # Use the existing ID if the entity is already in the mapping\n",
    "                else:\n",
    "                    entity2id[h] = ent_counter  # Assign a new ID to the entity\n",
    "                    ent_counter += 1\n",
    "                    hid = entity2id[h]\n",
    "\n",
    "                # Handle the tail entity\n",
    "                if t in entity2id:\n",
    "                    tid = entity2id[t]  # Use the existing ID if the entity is already in the mapping\n",
    "                else:\n",
    "                    entity2id[t] = ent_counter  # Assign a new ID to the entity\n",
    "                    ent_counter += 1\n",
    "                    tid = entity2id[t]\n",
    "\n",
    "                # Handle the relation\n",
    "                if r in relation2id:\n",
    "                    rid = relation2id[r]  # Use the existing ID if the relation is already in the mapping\n",
    "                else:\n",
    "                    relation2id[r] = rel_counter  # Assign a new ID to the relation\n",
    "                    rel_counter += 1\n",
    "                    rid = relation2id[r]\n",
    "\n",
    "                # Add the triple (head entity ID, tail entity ID, relation ID) to the triples dictionary\n",
    "                addTriple(triples, hid, tid, rid)\n",
    "\n",
    "            print('Relation:', rel_counter, ' Entity:', ent_counter)\n",
    "\n",
    "    # Return the dictionaries containing mappings and the triples\n",
    "    return entity2id, relation2id, triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e196f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe226b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the dataset is located\n",
    "folders = ['./data']\n",
    "\n",
    "# Define the filename pattern to identify the dataset files\n",
    "fileext = 'drugbank.nq.gz'\n",
    "\n",
    "# Call the 'preprocess' function to process the dataset and retrieve mappings and triples\n",
    "entity2id, relation2id, triples = preprocess(folders, fileext)\n",
    "\n",
    "# The 'entity2id' dictionary maps entities to unique IDs.\n",
    "# The 'relation2id' dictionary maps relations to unique IDs.\n",
    "# The 'triples' dictionary stores the processed triples (head entity ID, tail entity ID, relation ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file path of the first file matching the file extension in the folder\n",
    "filepath = None\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(fileext):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            break\n",
    "    if filepath:\n",
    "        break\n",
    "\n",
    "# open and decompress the file using gzip\n",
    "with gzip.open(filepath, 'rt') as f:\n",
    "    # read the first 10 lines of the file using rdflib\n",
    "    g = Graph()\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 60:\n",
    "            break\n",
    "        g.parse(data=line, format='nquads')\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69098eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the total number of triples\n",
    "num_triples = 0\n",
    "\n",
    "# Iterate through the 'triples' dictionary, where 'source' is a key\n",
    "for source in triples:\n",
    "    # For each 'source', iterate through the 'target' keys in its nested dictionary\n",
    "    for target in triples[source]:\n",
    "        # Increment 'num_triples' by the number of triples in the list associated with the current 'source' and 'target'\n",
    "        num_triples += len(triples[source][target])\n",
    "\n",
    "# Print the total number of triples\n",
    "print('Number of triples:', num_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named 'randomNWalkUniform' that takes the following parameters:\n",
    "# 'triples' - a dictionary containing triples\n",
    "# 'n' - the number of nodes or entities in the graph\n",
    "# 'walks' - the number of random walks to generate\n",
    "# 'path_depth' - the depth or length of each random walk\n",
    "def randomNWalkUniform(triples, n, walks, path_depth):\n",
    "    # Create an empty list to store the generated random walks\n",
    "    path = []\n",
    "    \n",
    "    # Perform 'walks' number of random walks\n",
    "    for k in range(walks):\n",
    "        # Generate a single random walk using the 'randomWalkUniform' function\n",
    "        walk = randomWalkUniform(triples, n, path_depth)\n",
    "        \n",
    "        # Append the generated walk to the 'path' list\n",
    "        path.append(walk)\n",
    "    \n",
    "    # Remove duplicate random walks by converting the list to a set and back to a list\n",
    "    path = list(set(path))\n",
    "    \n",
    "    # 'path' now represents the unique random walks generated by the function\n",
    "    \n",
    "    # Return the list of random walks as the output of the function\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe01192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of random walks to generate\n",
    "walks = 5\n",
    "# Define the depth or length of each random walk\n",
    "path_depth = 10\n",
    "# Generate random walks using the 'randomNWalkUniform' function\n",
    "# Parameters:\n",
    "# - 'triples' is the dictionary of triples\n",
    "# - '100' is the number of nodes or entities in the graph\n",
    "# - 'walks' is the number of random walks to generate\n",
    "# - 'path_depth' is the depth or length of each random walk\n",
    "paths = randomNWalkUniform(triples, 100, walks, path_depth)\n",
    "\n",
    "# Print each random walk path on a new line\n",
    "print('\\n'.join(paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of entities by extracting the values from the 'entity2id' dictionary\n",
    "entities = list(entity2id.values())\n",
    "\n",
    "# Broadcast the 'triples' dictionary using Spark's broadcast variable for efficient distributed processing\n",
    "# This allows 'triples' to be shared among different Spark tasks without being duplicated in memory\n",
    "b_triples = sc.broadcast(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd02abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the folder path where the random walk results will be saved\n",
    "folder = './walks/'\n",
    "\n",
    "# Check if the folder does not exist, and if so, create it\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "# Specify the number of random walks and the maximum depth of each walk\n",
    "walks = 250\n",
    "maxDepth = 5\n",
    "\n",
    "# Iterate through different path depths from 1 to (maxDepth - 1)\n",
    "for path_depth in range(1, maxDepth):\n",
    "    # Define the filename for the random walk results based on the current path depth\n",
    "    filename = folder + 'randwalkstest_n%d_depth%d_pagerank_uniform.txt' % (walks, path_depth)\n",
    "    \n",
    "    # Print the filename for the current path depth\n",
    "    print(filename)\n",
    "    \n",
    "    # Record the start time to measure the time taken for random walk generation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create an RDD (Resilient Distributed Dataset) from the 'entities' data and apply 'randomNWalkUniform' function to each entity\n",
    "    rdd = sc.parallelize(entities).flatMap(lambda n: randomNWalkUniform(b_triples.value, n, walks, path_depth))\n",
    "    \n",
    "    # Save the RDD as text files with the specified filename\n",
    "    rdd.saveAsTextFile(filename)\n",
    "    \n",
    "    # Calculate the elapsed time for random walk generation\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Print the elapsed time in the format HH:MM:SS\n",
    "    print('Time elapsed to generate features:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named 'saveData' that takes the following parameters:\n",
    "# - 'entity2id' - a dictionary containing mappings of entities to their IDs\n",
    "# - 'relation2id' - a dictionary containing mappings of relations to their IDs\n",
    "# - 'triples' - a nested dictionary containing triples\n",
    "# - 'dirname' - the directory where the data will be saved\n",
    "def saveData(entity2id, relation2id, triples, dirname):\n",
    "    # Check if the specified directory does not exist, and if so, create it\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)  \n",
    "    \n",
    "    # Open a file called \"entity2id.txt\" in write mode ('w') within the specified directory\n",
    "    entity2id_file = open(os.path.join(dirname, 'entity2id.txt'), 'w')\n",
    "    \n",
    "    # Open a file called \"relation2id.txt\" in write mode ('w') within the specified directory\n",
    "    relation2id_file = open(os.path.join(dirname, 'relation2id.txt'), 'w')\n",
    "    \n",
    "    # Open a file called \"train2id.txt\" in write mode ('w') within the specified directory\n",
    "    train_file = open(os.path.join(dirname, 'train2id.txt'), 'w')\n",
    "\n",
    "    # Write the total number of triples to the \"train2id.txt\" file followed by a newline character\n",
    "    train_file.write(str(num_triples) + '\\n') \n",
    "    \n",
    "    # Iterate through the triples in the 'triples' dictionary\n",
    "    for source in triples:\n",
    "        for target in triples[source]:\n",
    "            hid = source\n",
    "            tid = target\n",
    "            # Iterate through the relations associated with the current source and target\n",
    "            for rid in triples[source][target]:\n",
    "                # Write the head entity ID (hid), tail entity ID (tid), and relation ID (rid) to the \"train2id.txt\" file\n",
    "                train_file.write(\"%d %d %d\\n\" % (hid, tid, rid))\n",
    "\n",
    "    # Write the total number of entities (length of 'entity2id') to the \"entity2id.txt\" file followed by a newline character\n",
    "    entity2id_file.write(str(len(entity2id)) + '\\n')  \n",
    "    \n",
    "    # Iterate through entities in sorted order based on their IDs\n",
    "    for e in sorted(entity2id, key=entity2id.__getitem__):\n",
    "        # Write each entity and its corresponding ID to the \"entity2id.txt\" file\n",
    "        entity2id_file.write(e + '\\t' + str(entity2id[e]) + '\\n')  \n",
    "\n",
    "    # Write the total number of relations (length of 'relation2id') to the \"relation2id.txt\" file followed by a newline character\n",
    "    relation2id_file.write(str(len(relation2id)) + '\\n')    \n",
    "    \n",
    "    # Iterate through relations in sorted order based on their IDs\n",
    "    for r in sorted(relation2id, key=relation2id.__getitem__):\n",
    "        # Write each relation and its corresponding ID to the \"relation2id.txt\" file\n",
    "        relation2id_file.write(r + '\\t' + str(relation2id[r]) + '\\n') \n",
    "        \n",
    "    # Close the \"train2id.txt\" file\n",
    "    train_file.close()\n",
    "    \n",
    "    # Close the \"entity2id.txt\" file\n",
    "    entity2id_file.close()\n",
    "    \n",
    "    # Close the \"relation2id.txt\" file\n",
    "    relation2id_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e93712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where the data will be saved\n",
    "dirname = 'db_graph'\n",
    "\n",
    "# Call the 'saveData' function to save entity, relation, and triple data to the specified directory\n",
    "# Parameters:\n",
    "# - 'entity2id' - a dictionary containing mappings of entities to their IDs\n",
    "# - 'relation2id' - a dictionary containing mappings of relations to their IDs\n",
    "# - 'triples' - a nested dictionary containing triples\n",
    "# - 'dirname' - the directory where the data will be saved\n",
    "saveData(entity2id, relation2id, triples, dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8aedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Define a custom class named 'MySentences' for iterating over text data in a specific directory and filename\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname, filename):\n",
    "        # Initialize the class with the directory name and filename\n",
    "        self.dirname = dirname\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Print a message indicating that processing is starting for the specified filename\n",
    "        print('Processing', self.filename)\n",
    "        \n",
    "        # Iterate over files in the specified directory\n",
    "        for subfname in os.listdir(self.dirname):\n",
    "            # Check if the current subfile does not contain the specified filename and skip it\n",
    "            if not self.filename in subfname:\n",
    "                continue\n",
    "            \n",
    "            # Construct the full path to the subfile\n",
    "            fpath = os.path.join(self.dirname, subfname)\n",
    "            \n",
    "            # Iterate over files in the subdirectory\n",
    "            for fname in os.listdir(fpath):\n",
    "                # Check if the current file name does not contain 'part' and skip it\n",
    "                if not 'part' in fname:\n",
    "                    continue\n",
    "                \n",
    "                # Check if the current file name contains '.crc' and skip it\n",
    "                if '.crc' in fname:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Open and read each line in the current file\n",
    "                    for line in open(os.path.join(fpath, fname), mode='r'):\n",
    "                        # Remove the trailing newline character and split the line into words using \"->\" as the separator\n",
    "                        line = line.rstrip('\\n')\n",
    "                        words = line.split(\"->\")\n",
    "                        \n",
    "                        # Yield the words as an iterable\n",
    "                        yield words\n",
    "                except Exception:\n",
    "                    # Handle any exceptions that may occur while reading the file and print an error message\n",
    "                    print(\"Failed reading file:\")\n",
    "                    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783fd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named 'extractFeatureVector' that takes the following parameters:\n",
    "# - 'model' - a Gensim word embedding model\n",
    "# - 'drugs' - a list of drug IDs\n",
    "# - 'id2entity' - a dictionary mapping IDs to entity names\n",
    "# - 'output' - the name of the output file where feature vectors will be saved\n",
    "\n",
    "def extractFeatureVector(model, drugs, id2entity, output):\n",
    "    # Initialize the 'header' string with the column names starting with \"Entity\"\n",
    "    header = \"Entity\"\n",
    "    \n",
    "    # Define a namespace abbreviation, e.g., \"n\" for entities\n",
    "    ns = \"n\"\n",
    "    \n",
    "    # Construct the first entity column name by combining the namespace and the first drug ID\n",
    "    first = ns + str(drugs[0])\n",
    "    \n",
    "    # Iterate through the dimensions of the feature vectors in the model\n",
    "    for i in range(len(model.wv[first])):\n",
    "        # Append column names for each feature dimension, e.g., \"feature0\", \"feature1\", ...\n",
    "        header = header + \"\\tfeature\" + str(i)\n",
    "        \n",
    "    # Open the output file for writing\n",
    "    fw = open(output, 'w')\n",
    "    \n",
    "    # Write the header row to the output file\n",
    "    fw.write(header + \"\\n\")\n",
    "\n",
    "    # Iterate through drug IDs sorted in ascending order\n",
    "    for id_ in sorted(drugs):\n",
    "        # Create a namespace-specific ID, e.g., \"n123\" for the current drug ID\n",
    "        nid = ns + str(id_)\n",
    "        \n",
    "        # Check if the namespace-specific ID is not in the word embedding model\n",
    "        if nid not in model.wv:\n",
    "            # Print a message indicating the missing ID and continue to the next ID\n",
    "            print(nid)\n",
    "            continue\n",
    "        \n",
    "        # Retrieve the feature vector for the current namespace-specific ID from the model\n",
    "        vec = model.wv[nid]\n",
    "        \n",
    "        # Convert the feature vector to a string, with values separated by tabs\n",
    "        vec = \"\\t\".join(map(str, vec))\n",
    "        \n",
    "        # Write the entity name, followed by its feature vector, to the output file\n",
    "        fw.write(id2entity[id_] + '\\t' + str(vec) + '\\n')\n",
    "    \n",
    "    # Close the output file\n",
    "    fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf91086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named 'trainModel' that takes the following parameters:\n",
    "# - 'drugs' - a list of drug IDs\n",
    "# - 'id2entity' - a dictionary mapping IDs to entity names\n",
    "# - 'datafilename' - the directory containing the data for training the model\n",
    "# - 'model_output' - the directory where the trained models will be saved\n",
    "# - 'vector_output' - the directory where feature vectors will be saved\n",
    "# - 'pattern' - a string specifying the pattern (e.g., 'uniform')\n",
    "# - 'maxDepth' - the maximum depth for random walks\n",
    "def trainModel(drugs, id2entity, datafilename, model_output, vector_output, pattern, maxDepth):\n",
    "    # Check if the 'model_output' directory does not exist, and if not, create it\n",
    "    if not os.path.isdir(model_output):\n",
    "        os.mkdir(model_output)\n",
    "        \n",
    "    # Check if the 'vector_output' directory does not exist, and if not, create it\n",
    "    if not os.path.isdir(vector_output):\n",
    "        os.mkdir(vector_output)\n",
    "    \n",
    "    # Create an 'output' directory specific to the 'pattern' within 'model_output'\n",
    "    output = model_output + pattern + '/'\n",
    "    if not os.path.isdir(output):\n",
    "        os.mkdir(output)\n",
    "    \n",
    "    # Define a custom iterator 'sentences' for processing text data\n",
    "    sentences = MySentences(datafilename, filename=pattern) # a memory-friendly iterator\n",
    "    \n",
    "    # Initialize a Gensim Word2Vec model with specific parameters\n",
    "    model = gensim.models.Word2Vec(size=200, workers=5, window=5, sg=1, negative=15, iter=5)\n",
    "    \n",
    "    # Build the vocabulary of the model using the custom iterator 'sentences'\n",
    "    model.build_vocab(sentences)\n",
    "    \n",
    "    # Store the total count of sentences in the corpus\n",
    "    corpus_count = model.corpus_count\n",
    "    \n",
    "    # Delete the model to save memory\n",
    "    del model\n",
    "    \n",
    "    # Initialize another Gensim Word2Vec model with different parameters\n",
    "    model1 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=1, negative=15, iter=5)\n",
    "    \n",
    "    # Build the vocabulary of the model using the custom iterator 'sentences'\n",
    "    model1.build_vocab(sentences)\n",
    "    \n",
    "    # Train the model on the sentences\n",
    "    model1.train(sentences, total_examples=corpus_count, epochs=5)\n",
    "    \n",
    "    # Define a model name based on parameters\n",
    "    modelname = 'Entity2Vec_sg_200_5_5_15_2_500' + '_d' + str(maxDepth)\n",
    "    \n",
    "    # Save the trained model to the 'output' directory\n",
    "    model1.save(output + modelname)\n",
    "    \n",
    "    # Extract and save feature vectors using the 'extractFeatureVector' function\n",
    "    extractFeatureVector(model1, drugs, id2entity, vector_output + modelname + '_' + pattern + '.txt')\n",
    "    \n",
    "    # Delete the model to save memory\n",
    "    del model1\n",
    "    \n",
    "    # Initialize another Gensim Word2Vec model with different parameters (CBOW)\n",
    "    model2 = gensim.models.Word2Vec(size=200, workers=8, window=5, sg=0, iter=5, cbow_mean=1, alpha=0.05)\n",
    "    \n",
    "    # Build the vocabulary of the model using the custom iterator 'sentences'\n",
    "    model2.build_vocab(sentences)\n",
    "    \n",
    "    # Train the model on the sentences\n",
    "    model2.train(sentences, total_examples=corpus_count, epochs=5)\n",
    "    \n",
    "    # Define a model name based on parameters\n",
    "    modelname = 'Entity2Vec_cbow_200_5_5_2_500' + '_d' + str(maxDepth)\n",
    "    \n",
    "    # Save the trained CBOW model to the 'output' directory\n",
    "    model2.save(output + modelname)\n",
    "    \n",
    "    # Extract and save feature vectors using the 'extractFeatureVector' function\n",
    "    extractFeatureVector(model2, drugs, id2entity, vector_output + modelname + '_' + pattern + '.txt')\n",
    "    \n",
    "    # Delete the model to save memory\n",
    "    del model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_df = pd.read_csv('data/input/ddi_v5.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7020dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_ns ='http://bio2rdf.org/drugbank:'\n",
    "#The db_ns variable is a string that stores the base URI of the DrugBank ontology. \n",
    "#The ddi_df.Drug1 and ddi_df.Drug2 columns contain the names of the two drugs involved in each DDI.\n",
    "ddi_df.Drug1 = '<'+db_ns+ddi_df.Drug1+'>'\n",
    "ddi_df.Drug2 = '<'+db_ns+ddi_df.Drug2+'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e648515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty set called 'db_entities' to store DrugBank entities\n",
    "db_entities = set()\n",
    "\n",
    "# Create a set called 'drugs' containing unique Drug1 and Drug2 values from the 'ddi_df' DataFrame\n",
    "drugs = set(ddi_df.Drug1.unique()).union(ddi_df.Drug2.unique())\n",
    "\n",
    "# Iterate through each 'dbid' (DrugBank ID) in the 'drugs' set\n",
    "for dbid in drugs:\n",
    "    # Check if the 'dbid' is present in the 'entity2id' dictionary\n",
    "    if dbid in entity2id:\n",
    "        # If the 'dbid' is found in 'entity2id', add its corresponding ID to the 'db_entities' set\n",
    "        db_entities.add(entity2id[dbid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806428f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_entities =list(db_entities)\n",
    "print (len(db_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed56226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary called 'id2entity' by reversing the key-value pairs of the 'entity2id' dictionary\n",
    "id2entity = {value: key for key, value in entity2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d13f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the data for training the model\n",
    "datafilename = './walks/'\n",
    "\n",
    "# Define the directory where the trained models will be saved\n",
    "model_output = './models/'\n",
    "\n",
    "# Define the pattern (e.g., 'uniform') for naming the models and vectors\n",
    "pattern = 'uniform'\n",
    "\n",
    "# Define the directory where feature vectors will be saved\n",
    "vector_output = './vectors/'\n",
    "\n",
    "# Call the 'trainModel' function to train the models and extract feature vectors\n",
    "# Parameters:\n",
    "# - 'db_entities' - set of DrugBank entity IDs\n",
    "# - 'id2entity' - dictionary mapping entity IDs to entity names\n",
    "# - 'datafilename' - directory containing the data for training\n",
    "# - 'model_output' - directory to save trained models\n",
    "# - 'vector_output' - directory to save feature vectors\n",
    "# - 'pattern' - pattern for model and vector names\n",
    "# - 'maxDepth' - maximum depth for random walks (assuming 'maxDepth' is defined elsewhere)\n",
    "trainModel(db_entities, id2entity, datafilename, model_output, vector_output, pattern, maxDepth)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
